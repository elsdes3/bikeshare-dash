{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5b0bb5d-db19-4724-970f-6eb9f1a30654",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21a3f2d-254f-472e-b7bd-ccb5c68db66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9f9db8-5371-4589-9e49-3d350067afd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from glob import glob\n",
    "from io import BytesIO\n",
    "from multiprocessing import cpu_count\n",
    "from typing import Dict, List, Union\n",
    "from urllib.request import urlopen\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import boto3\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import requests\n",
    "import snowflake.connector\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from joblib import Parallel, delayed\n",
    "from snowflake.connector.pandas_tools import write_pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1911fddb-1554-453b-8039-f6abc043fbc0",
   "metadata": {},
   "source": [
    "## About"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16880c3-3e22-401b-ac46-2f08637fad0e",
   "metadata": {},
   "source": [
    "Download Toronto Bikeshare trips data, bikeshare stations metadata and supplementary (neighbourhood-specific) datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576ef8a1-8def-48b9-8836-be4d1b09450f",
   "metadata": {},
   "source": [
    "## User Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8179f53-54ba-4b62-a158-2e1cd157f3e0",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Datasets\n",
    "# # Open Data Portal\n",
    "url = \"https://ckan0.cf.opendata.inter.prod-toronto.ca/api/3/action/package_show\"\n",
    "# # Ridership\n",
    "params = {\"id\": \"7e876c24-177c-4605-9cef-e50dd74c617f\"}\n",
    "years_wanted = {2021: list(range(1, 12 + 1)), 2022: list(range(1, 1 + 1))}\n",
    "# # Stations Metadata\n",
    "about_params = {\"id\": \"2b44db0d-eea9-442d-b038-79335368ad5a\"}\n",
    "stations_cols_wanted = [\n",
    "    \"station_id\",\n",
    "    \"name\",\n",
    "    \"physical_configuration\",\n",
    "    \"lat\",\n",
    "    \"lon\",\n",
    "    \"altitude\",\n",
    "    \"address\",\n",
    "    \"capacity\",\n",
    "    \"physicalkey\",\n",
    "    \"transitcard\",\n",
    "    \"creditcard\",\n",
    "    \"phone\",\n",
    "]\n",
    "\n",
    "# Ridership datetime columns\n",
    "date_cols = [\"Start Time\", \"End Time\"]\n",
    "\n",
    "# Ridership columns in which to drop missing values\n",
    "nan_cols = [\n",
    "    \"Start Station Id\",\n",
    "    \"End Station Id\",\n",
    "    \"Start Station Name\",\n",
    "    \"End Station Name\",\n",
    "]\n",
    "\n",
    "# Snowflake resources\n",
    "# # Database\n",
    "stations_db_name = \"torbikestations\"\n",
    "# # Tables\n",
    "trips_table_name = \"trips\"\n",
    "station_stats_table_name = \"station_stats\"\n",
    "# # Stage\n",
    "trips_stage_name = \"bikes_stage\"\n",
    "# # File Format\n",
    "trips_file_format_name = \"COMMASEP_ONEHEADROW\"\n",
    "\n",
    "ci_run = \"no\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd24cad-94d4-466f-8399-93c38d4996dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridership dtypes dict\n",
    "dtypes_dict = {\n",
    "    \"Trip Id\": pd.Int64Dtype(),\n",
    "    \"Trip Duration\": pd.Int64Dtype(),\n",
    "    \"Start Station Id\": pd.Int64Dtype(),\n",
    "    \"Start Station Name\": pd.StringDtype(),\n",
    "    \"Start Station Id\": pd.Int64Dtype(),\n",
    "    \"Start Station Name\": pd.StringDtype(),\n",
    "    \"Bike Id\": pd.Float64Dtype(),\n",
    "    \"User Type\": pd.StringDtype(),\n",
    "}\n",
    "\n",
    "if ci_run == \"no\":\n",
    "    load_dotenv(find_dotenv())\n",
    "\n",
    "trips_db_name = os.getenv(\"DB_NAME\")\n",
    "snowflake_dict_no_db = dict(\n",
    "    user=os.getenv(\"SNOWFLAKE_USER\"),\n",
    "    password=os.getenv(\"SNOWFLAKE_PASS\"),\n",
    "    account=os.getenv(\"SNOWFLAKE_ACCOUNT\"),\n",
    "    warehouse=os.getenv(\"SNOWFLAKE_WAREHOUSE\"),\n",
    "    role=\"sysadmin\",\n",
    ")\n",
    "snowflake_dict = dict(\n",
    "    user=os.getenv(\"SNOWFLAKE_USER\"),\n",
    "    password=os.getenv(\"SNOWFLAKE_PASS\"),\n",
    "    account=os.getenv(\"SNOWFLAKE_ACCOUNT\"),\n",
    "    warehouse=os.getenv(\"SNOWFLAKE_WAREHOUSE\"),\n",
    "    database=trips_db_name,\n",
    "    schema=os.getenv(\"SNOWFLAKE_DB_SCHEMA\"),\n",
    "    role=\"sysadmin\",\n",
    ")\n",
    "snowflake_station_stats_dict = dict(\n",
    "    user=os.getenv(\"SNOWFLAKE_USER\"),\n",
    "    password=os.getenv(\"SNOWFLAKE_PASS\"),\n",
    "    account=os.getenv(\"SNOWFLAKE_ACCOUNT\"),\n",
    "    warehouse=os.getenv(\"SNOWFLAKE_WAREHOUSE\"),\n",
    "    database=stations_db_name,\n",
    "    schema=os.getenv(\"SNOWFLAKE_DB_SCHEMA\"),\n",
    "    role=\"sysadmin\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f041ef9-240f-4efb-9270-fd6f64b03901",
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_region = os.getenv(\"AWS_REGION\")\n",
    "account_id = (\n",
    "    boto3.client(\"sts\", region_name=aws_region).get_caller_identity().get(\"Account\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03a2ddc-0082-4ade-a023-a8f82753639b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_urls(\n",
    "    main_dataset_url: str, dataset_params: Dict, years_wanted: Dict[int, List]\n",
    ") -> List:\n",
    "    package = requests.get(main_dataset_url, params=dataset_params).json()\n",
    "    resources = package[\"result\"][\"resources\"]\n",
    "    df = pd.DataFrame.from_records(resources)\n",
    "    year_month_wanted = [\n",
    "        f\"{y}-{str(m).zfill(2)}\" for y, ms in years_wanted.items() for m in ms\n",
    "    ]\n",
    "    year_month_wanted_str = \"|\".join(year_month_wanted)\n",
    "    urls_list = df.query(\"name.str.contains(@year_month_wanted_str)\")[\"url\"].tolist()\n",
    "    return urls_list\n",
    "\n",
    "\n",
    "def read_data(\n",
    "    url: str, dtypes_dict: Dict, date_cols: List[str], nan_cols: List[str]\n",
    ") -> Dict[str, Union[List[str], int]]:\n",
    "    df = pd.read_csv(\n",
    "        url,\n",
    "        encoding=\"cp1252\",\n",
    "        parse_dates=date_cols,\n",
    "        dtype=dtypes_dict,\n",
    "    ).dropna(subset=nan_cols)\n",
    "    # df.columns = [re.sub(\"[^A-Za-z0-9\\s]+\", \"\", c) for c in list(df)]\n",
    "    df.columns = [\n",
    "        re.sub(\"[^A-Za-z0-9\\s]+\", \"\", c).replace(\" \", \"_\").upper() for c in list(df)\n",
    "    ]\n",
    "    df.columns = df.columns.str.replace(\" \", \"_\").str.upper()\n",
    "    fpath = f\"data/raw/{os.path.basename(url).replace('.csv', '')}.csv\"\n",
    "    if not os.path.exists(fpath):\n",
    "        df.to_csv(fpath, index=False)\n",
    "    return {os.path.basename(url): {\"columns\": list(df), \"nrows\": len(df)}}\n",
    "\n",
    "\n",
    "def get_single_ridership_data_file(\n",
    "    url: str, dtypes_dict: Dict, date_cols: List[str], nan_cols: List[str]\n",
    ") -> Dict[str, List[str]]:\n",
    "    fname = os.path.basename(url)\n",
    "    print(f\"Loading data from {fname}...\", end=\"\")\n",
    "    cols_dict = read_data(url, dtypes_dict, date_cols, nan_cols)\n",
    "    print(\"Done.\")\n",
    "    return cols_dict\n",
    "\n",
    "\n",
    "def get_all_data_files(\n",
    "    urls_list: List, dtypes_dict: Dict, date_cols: List[str], nan_cols: List[str]\n",
    ") -> Dict[str, List[str]]:\n",
    "    executor = Parallel(n_jobs=cpu_count(), backend=\"multiprocessing\")\n",
    "    tasks = (\n",
    "        delayed(get_single_ridership_data_file)(url, dtypes_dict, date_cols, nan_cols)\n",
    "        for url in urls_list\n",
    "    )\n",
    "    cols_dicts = executor(tasks)\n",
    "    # cols_dicts = [\n",
    "    #     get_single_ridership_data_file(url, dtypes_dict, date_cols, nan_cols)\n",
    "    #     for url in urls_list\n",
    "    # ]\n",
    "    return cols_dicts\n",
    "\n",
    "\n",
    "def get_stations_metadata(stations_url: str, stations_params: Dict) -> pd.DataFrame:\n",
    "    package = requests.get(stations_url, params=about_params).json()\n",
    "    resources = package[\"result\"][\"resources\"]\n",
    "    df_about = pd.DataFrame.from_records(resources)\n",
    "    r = requests.get(df_about[\"url\"].tolist()[0]).json()\n",
    "    url_stations = r[\"data\"][\"en\"][\"feeds\"][2][\"url\"]\n",
    "    df_stations = pd.DataFrame.from_records(\n",
    "        requests.get(url_stations).json()[\"data\"][\"stations\"]\n",
    "    )\n",
    "    return df_stations\n",
    "\n",
    "\n",
    "def transform_metadata(\n",
    "    df: pd.DataFrame, stations_cols_wanted: List[str]\n",
    ") -> pd.DataFrame:\n",
    "    df[\"station_id\"] = df[\"station_id\"].astype(int)\n",
    "    dfa = pd.DataFrame(\n",
    "        df.set_index(\"station_id\")[\"rental_methods\"].tolist(),\n",
    "        columns=[\"key\", \"transitcard\", \"creditcard\", \"phone\"],\n",
    "    )\n",
    "    for c in [\"KEY\", \"TRANSITCARD\", \"CREDITCARD\", \"PHONE\"]:\n",
    "        dfa[c.lower()] = dfa[c.lower()].map({c: 1}).fillna(0).astype(int)\n",
    "    df = pd.concat(\n",
    "        [\n",
    "            df.drop(columns=[\"groups\", \"rental_methods\"]),\n",
    "            dfa,\n",
    "        ],\n",
    "        axis=1,\n",
    "    ).rename(columns={\"key\": \"physicalkey\"})[stations_cols_wanted]\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_toronto_open_data(url, params, col_rename_dict={}):\n",
    "    package = requests.get(url, params=params).json()\n",
    "    datastore_url = (\n",
    "        \"https://ckan0.cf.opendata.inter.prod-toronto.ca/api/3/\"\n",
    "        \"action/datastore_search\"\n",
    "    )\n",
    "    for _, resource in enumerate(package[\"result\"][\"resources\"]):\n",
    "        if resource[\"datastore_active\"]:\n",
    "            url = datastore_url\n",
    "            p = {\"id\": resource[\"id\"]}\n",
    "            data = requests.get(url, params=p).json()\n",
    "            df = pd.DataFrame(data[\"result\"][\"records\"])\n",
    "            break\n",
    "    if col_rename_dict:\n",
    "        df = df.rename(columns=col_rename_dict)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_lat_long(row):\n",
    "    return row[\"coordinates\"]\n",
    "\n",
    "\n",
    "def get_poi_data(url: str, params: Dict) -> pd.DataFrame:\n",
    "    poi_cols = [\n",
    "        \"ID\",\n",
    "        \"NAME\",\n",
    "        \"PLACE_NAME\",\n",
    "        \"ADDRESS_FULL\",\n",
    "        \"POSTAL_CODE\",\n",
    "        \"ATTRACTION_DESC\",\n",
    "        \"POI_LATITUDE\",\n",
    "        \"POI_LONGITUDE\",\n",
    "    ]\n",
    "    package = requests.get(url, params=poi_params).json()\n",
    "    poi_url = package[\"result\"][\"resources\"][0][\"url\"]\n",
    "    df = pd.read_csv(poi_url)\n",
    "    df = df.rename(columns={list(df)[0]: \"ID\"})\n",
    "\n",
    "    df[[\"POI_LONGITUDE\", \"POI_LATITUDE\"]] = pd.DataFrame(\n",
    "        df[\"geometry\"].apply(eval).apply(get_lat_long).tolist()\n",
    "    )\n",
    "    # Verify no duplicates (by name) are in the data\n",
    "    assert df[df.duplicated(subset=[\"NAME\"], keep=False)].empty\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_cultural_hotspots(url: str, params: Dict) -> pd.DataFrame:\n",
    "    package = requests.get(url, params=params).json()\n",
    "    ch_locations = package[\"result\"][\"resources\"][0][\"url\"]\n",
    "    ch_locs_dir_path = \"data/raw/cultural-hotspot-points-of-interest-wgs84\"\n",
    "    with urlopen(ch_locations) as zipresp:\n",
    "        with ZipFile(BytesIO(zipresp.read())) as zfile:\n",
    "            zfile.extractall(ch_locs_dir_path)\n",
    "    df = gpd.read_file(f\"{ch_locs_dir_path}/CULTURAL_HOTSPOT_WGS84.shp\")\n",
    "    df = (\n",
    "        df.drop_duplicates(\n",
    "            subset=[\"PNT_OF_INT\", \"LATITUDE\", \"LONGITUDE\"],\n",
    "            keep=\"first\",\n",
    "        )\n",
    "        .reset_index(drop=True)\n",
    "        .copy()\n",
    "    )\n",
    "    df = (\n",
    "        df.drop_duplicates(\n",
    "            subset=[\"PNT_OF_INT\"],\n",
    "            keep=\"first\",\n",
    "        )\n",
    "        .reset_index(drop=True)\n",
    "        .copy()\n",
    "    )\n",
    "    assert df[df.duplicated(subset=[\"PNT_OF_INT\"], keep=False)].empty\n",
    "    df_essentials = df[[\"RID\", \"PNT_OF_INT\", \"LATITUDE\", \"LONGITUDE\"]].rename(\n",
    "        columns={\n",
    "            \"RID\": \"ID\",\n",
    "            \"PNT_OF_INT\": \"NAME\",\n",
    "            \"LATITUDE\": \"POI_LATITUDE\",\n",
    "            \"LONGITUDE\": \"POI_LONGITUDE\",\n",
    "        }\n",
    "    )\n",
    "    return df_essentials\n",
    "\n",
    "\n",
    "def get_neighbourhood_boundary_land_area_data(url: str, params: Dict) -> pd.DataFrame:\n",
    "    package = requests.get(url, params=params).json()\n",
    "    files = package[\"result\"][\"resources\"]\n",
    "    n_url = [f[\"url\"] for f in files if f[\"url\"].endswith(\"4326.geojson\")][0]\n",
    "    gdf = gpd.read_file(n_url)\n",
    "    gdf[\"centroid\"] = gdf[\"geometry\"].to_crs(epsg=3395).centroid.to_crs(epsg=4326)\n",
    "    gdf[\"AREA_LATITUDE\"] = gdf[\"centroid\"].y\n",
    "    gdf[\"AREA_LONGITUDE\"] = gdf[\"centroid\"].x\n",
    "    assert len(gdf) == 140\n",
    "    neigh_cols_to_show = [\n",
    "        \"AREA_ID\",\n",
    "        \"AREA_SHORT_CODE\",\n",
    "        \"AREA_LONG_CODE\",\n",
    "        \"AREA_NAME\",\n",
    "        \"Shape__Area\",\n",
    "        \"LATITUDE\",\n",
    "        \"AREA_LATITUDE\",\n",
    "        \"LONGITUDE\",\n",
    "        \"AREA_LONGITUDE\",\n",
    "    ]\n",
    "    return gdf\n",
    "\n",
    "\n",
    "def get_public_transit_locations(url: str, params: Dict) -> pd.DataFrame:\n",
    "    package = requests.get(url, params=params).json()\n",
    "    pt_locations = package[\"result\"][\"resources\"][0][\"url\"]\n",
    "    pt_locs_dir_path = \"data/raw/opendata_ttc_schedules\"\n",
    "    with urlopen(pt_locations) as zipresp:\n",
    "        with ZipFile(BytesIO(zipresp.read())) as zfile:\n",
    "            zfile.extractall(pt_locs_dir_path)\n",
    "    df_pt = pd.read_csv(f\"{pt_locs_dir_path}/stops.txt\")\n",
    "    display(df_pt.head())\n",
    "    df_pt = df_pt.rename(columns={\"stop_lat\": \"lat\", \"stop_lon\": \"lon\"})\n",
    "    return df_pt\n",
    "\n",
    "\n",
    "def get_coll_univ_locations() -> pd.DataFrame:\n",
    "    coll_univ_locations = {\n",
    "        \"centennial\": {\"lat\": 43.7854, \"lon\": -79.22664},\n",
    "        \"george-brown\": {\"lat\": 43.6761, \"lon\": -79.4111},\n",
    "        \"humber\": {\"lat\": 43.7290, \"lon\": -79.6074},\n",
    "        \"ocad\": {\"lat\": 43.6530, \"lon\": -79.3912},\n",
    "        \"ryerson\": {\"lat\": 43.6577, \"lon\": -79.3788},\n",
    "        \"seneca\": {\"lat\": 43.7955, \"lon\": -79.3496},\n",
    "        \"tynedale\": {\"lat\": 43.7970, \"lon\": -79.3945},\n",
    "        \"uoft-scarborough\": {\"lat\": 43.7844, \"lon\": -79.1851},\n",
    "        \"uoft\": {\"lat\": 43.6629, \"lon\": -79.5019},\n",
    "        \"yorku\": {\"lat\": 43.7735, \"lon\": -79.5019},\n",
    "        \"yorku-glendon\": {\"lat\": 43.7279, \"lon\": -79.3780},\n",
    "    }\n",
    "    df_coll_univ = (\n",
    "        pd.DataFrame.from_dict(coll_univ_locations, orient=\"index\")\n",
    "        .reset_index()\n",
    "        .rename(columns={\"index\": \"institution_name\"})\n",
    "        .reset_index()\n",
    "        .rename(columns={\"index\": \"institution_id\"})\n",
    "    )\n",
    "    return df_coll_univ\n",
    "\n",
    "\n",
    "def get_neighbourhood_profile_data(url: str, params: Dict) -> pd.DataFrame:\n",
    "    df_neigh_demog = get_toronto_open_data(url, params)\n",
    "    df_neigh_demog = (\n",
    "        df_neigh_demog[\n",
    "            df_neigh_demog[\"Characteristic\"].isin(\n",
    "                [\n",
    "                    \"Neighbourhood Number\",\n",
    "                    \"Youth (15-24 years)\",\n",
    "                    \"Working Age (25-54 years)\",\n",
    "                    \"Population, 2016\",\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "        .iloc[:, slice(4, None)]\n",
    "        .set_index(\"Characteristic\")\n",
    "        .T.reset_index()\n",
    "        .iloc[1:]\n",
    "        .reset_index(drop=True)\n",
    "        .rename(columns={\"index\": \"name\"})\n",
    "    )\n",
    "    assert len(df_neigh_demog) == 140\n",
    "    df_neigh_demog[\"AREA_NAME\"] = (\n",
    "        df_neigh_demog[\"name\"] + \" (\" + df_neigh_demog[\"Neighbourhood Number\"] + \")\"\n",
    "    )\n",
    "    return df_neigh_demog\n",
    "\n",
    "\n",
    "def get_neighbourhood_containing_point(\n",
    "    gdf: gpd.GeoDataFrame,\n",
    "    df: pd.DataFrame,\n",
    "    lat: str = \"Latitude\",\n",
    "    lon: str = \"Longitude\",\n",
    "    crs: int = 4326,\n",
    ") -> gpd.GeoDataFrame:\n",
    "    cols_order = list(df) + list(gdf)\n",
    "    polygons_contains = (\n",
    "        gpd.sjoin(\n",
    "            gdf,\n",
    "            gpd.GeoDataFrame(\n",
    "                df, geometry=gpd.points_from_xy(df[lon], df[lat]), crs=crs\n",
    "            ),\n",
    "            predicate=\"contains\",\n",
    "        )\n",
    "        .reset_index(drop=True)\n",
    "        .drop(columns=[\"index_right\"])[cols_order]\n",
    "    )\n",
    "    # print(polygons_contains)\n",
    "    return polygons_contains\n",
    "\n",
    "\n",
    "def get_data_with_neighbourhood(\n",
    "    gdf: gpd.GeoDataFrame,\n",
    "    df: pd.DataFrame,\n",
    "    lat: int,\n",
    "    lon: int,\n",
    "    col_to_join: str,\n",
    "    crs: int = 4326,\n",
    ") -> gpd.GeoDataFrame:\n",
    "    cols_to_keep = [col_to_join, \"AREA_NAME\", \"geometry\", \"Shape__Area\"]\n",
    "    df_check = get_neighbourhood_containing_point(gdf, df, lat, lon, crs)[cols_to_keep]\n",
    "    display(df_check.head(2))\n",
    "    df = df.merge(df_check.drop(columns=[\"geometry\"]), on=col_to_join, how=\"left\").drop(\n",
    "        columns=[\"geometry\"]\n",
    "    )\n",
    "    print(\n",
    "        f\"Dropped {len(df[['AREA_NAME']].isna().sum())} rows with a missing AREA_NAME\"\n",
    "    )\n",
    "    df = df.dropna(subset=[\"AREA_NAME\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "def summarize_df(df: pd.DataFrame) -> None:\n",
    "    \"\"\"Show properties of a DataFrame.\"\"\"\n",
    "    display(\n",
    "        df.dtypes.rename(\"dtype\")\n",
    "        .to_frame()\n",
    "        .merge(\n",
    "            df.isna().sum().rename(\"num_missing\").to_frame(),\n",
    "            left_index=True,\n",
    "            right_index=True,\n",
    "            how=\"left\",\n",
    "        )\n",
    "        .assign(num=len(df))\n",
    "        .merge(\n",
    "            df.nunique().rename(\"nunique\").to_frame(),\n",
    "            left_index=True,\n",
    "            right_index=True,\n",
    "            how=\"left\",\n",
    "        )\n",
    "        .merge(\n",
    "            df.dropna(how=\"any\")\n",
    "            .sample(1)\n",
    "            .squeeze()\n",
    "            .rename(\"single_non_nan_value\")\n",
    "            .to_frame(),\n",
    "            left_index=True,\n",
    "            right_index=True,\n",
    "            how=\"left\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def show_sql_df(\n",
    "    query: str,\n",
    "    cursor,\n",
    "    table_output: bool = False,\n",
    ") -> Union[None, pd.DataFrame]:\n",
    "    cursor.execute(query)\n",
    "    if table_output:\n",
    "        colnames = [cdesc[0].lower() for cdesc in cursor.description]\n",
    "        cur_fetched = cursor.fetchall()\n",
    "        if cur_fetched:\n",
    "            df_query_output = pd.DataFrame.from_records(cur_fetched, columns=colnames)\n",
    "            display(df_query_output)\n",
    "            return df_query_output\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9092f40-a70a-4c34-942e-55586d69d779",
   "metadata": {},
   "source": [
    "## Create AWS Python SDK Objects for Creating QuickSight Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a4f200-e81a-4818-b5ba-f4ca857054fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "qs_client_user = boto3.client(\"quicksight\", region_name=\"us-east-1\")\n",
    "qs_client = boto3.client(\"quicksight\", region_name=aws_region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c1d2ef-2a04-4cb4-a4ea-2614d8f42ef3",
   "metadata": {},
   "source": [
    "## Get Bikeshare Trips Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6bc687-50c4-424e-b143-6e314446db21",
   "metadata": {},
   "source": [
    "### Get URLs for Raw Trips Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e026ec-dfeb-4344-8186-317ad676b59a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "all_urls = get_file_urls(url, params, years_wanted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33513a23-c7f0-4950-979c-90ffcf18805d",
   "metadata": {},
   "source": [
    "### Download Raw Trips Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91de700c-3606-467c-89d9-baca9537d02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cols_dict_list = get_all_data_files(all_urls, dtypes_dict, date_cols, nan_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb85dbc8-1d10-4968-ba61-2be660806181",
   "metadata": {},
   "source": [
    "Perform sanity checks on column names and column order in raw trips data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e63e6ff-d904-4021-b131-aa29a3b45d5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cols_cleaned = {\n",
    "    k: [re.sub(\"[^A-Za-z0-9\\s]+\", \"\", c) for c in l[\"columns\"]]\n",
    "    for f in cols_dict_list\n",
    "    for k, l in f.items()\n",
    "}\n",
    "assert len(cols_cleaned) == len(cols_dict_list)\n",
    "\n",
    "cols_equality_checks = {\n",
    "    k: True if cols == list(cols_cleaned.values())[0] else False\n",
    "    for k, cols in {m: cols_cleaned[m] for m in list(cols_cleaned)[1:]}.items()\n",
    "}\n",
    "try:\n",
    "    assert all(list(cols_equality_checks.values()))\n",
    "except AssertionError:\n",
    "    print(cols_equality_checks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531951b3-3473-4d39-9105-bfe2439e7706",
   "metadata": {},
   "source": [
    "## Get Supplementary Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c25231-8a95-44ca-90bf-8a74e8c53241",
   "metadata": {},
   "source": [
    "### Stations Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e48b10c-2739-4a81-96df-e476bc33d4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_stations = get_stations_metadata(url, about_params)\n",
    "df_stations = transform_metadata(df_stations, stations_cols_wanted)\n",
    "display(df_stations.head(2))\n",
    "summarize_df(df_stations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020ca363-156c-4b8e-b375-02db4750a9a0",
   "metadata": {},
   "source": [
    "### Cultural Hotspots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af402b4c-109f-4f65-8660-7ad895e33f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "params = {\"id\": \"c7be2ee7-d317-4a28-8cbe-bff1ce116b46\"}\n",
    "dfch_essentials = get_cultural_hotspots(url, params)\n",
    "dfch_essentials.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f950dd5c-caa8-4838-a692-00c6f2ee0f8b",
   "metadata": {},
   "source": [
    "### Places of Interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb0ef0d-5f64-4be2-96ed-b39f46a21fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "poi_params = {\"id\": \"965247c0-c72e-49b4-bb1a-879cf98e1a32\"}\n",
    "df_poi = get_poi_data(url, poi_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22faaee9-52f1-4e4b-ad1b-a1d3d2815972",
   "metadata": {},
   "source": [
    "Note that duplicate lat-long will be permitted here as multiple places of interest may share the same physical location, or immediately adjacent area. Some examples of such places of interest with a duplicated latitude and longitde are shown in `0_get_bikeshare_data.ipynb`. So, the duplicate lat-long sites will be retained in this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ab6884-2433-4bbe-9def-6395e6f9ff91",
   "metadata": {},
   "source": [
    "### Neighbourhood Boundary and Land Area Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc8eceb-edf1-4329-8430-8831eb7b8986",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "neigh_params = {\"id\": \"4def3f65-2a65-4a4f-83c4-b2a4aed72d46\"}\n",
    "gdf = get_neighbourhood_boundary_land_area_data(url, neigh_params)\n",
    "neigh_cols_to_show = [\n",
    "    \"AREA_ID\",\n",
    "    \"AREA_SHORT_CODE\",\n",
    "    \"AREA_LONG_CODE\",\n",
    "    \"AREA_NAME\",\n",
    "    \"Shape__Area\",\n",
    "    \"LATITUDE\",\n",
    "    \"AREA_LATITUDE\",\n",
    "    \"LONGITUDE\",\n",
    "    \"AREA_LONGITUDE\",\n",
    "]\n",
    "gdf[\n",
    "    gdf[\"AREA_NAME\"].str.contains(\n",
    "        \"Wychwood|Yonge-Eglinton|Yonge-St.|York Univ|Yorkdale-Glen\"\n",
    "    )\n",
    "][neigh_cols_to_show].sort_values(by=[\"AREA_NAME\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796dac34-96d4-4098-9edb-f3235e9acfe1",
   "metadata": {},
   "source": [
    "In order to use the correct CRS for allowing an area calculation in square km, we'll get the current EPSG ([link](https://epsg.io/4326)) from the geodata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471ef829-9389-4ca6-a7ab-002afa967d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gdf.crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f03d88-adbc-43fa-b82e-e4c79d249260",
   "metadata": {},
   "source": [
    "Fix typographic errors in the name of the neighbourhood in this dataset\n",
    "- [North St. James Town](https://www.toronto.ca/ext/sdfa/Neighbourhood%20Profiles/pdf/2016/pdf1/cpa74.pdf) and [Cabbagetown-South St. James Town](https://www.toronto.com/community-static/4550668-cabbagetown-south-st-james-town/)\n",
    "  - missing space between ...St. and Ja...\n",
    "- Weston-Pelham Park\n",
    "  - incorrectly listed as its old name (from 2011) of Weston-Pellam Park ([link](https://www.toronto.ca/wp-content/uploads/2017/11/900b-91-Weston-Pellam-Park.pdf))\n",
    "  - replace with [new name from 2016](https://www.toronto.ca/ext/sdfa/Neighbourhood%20Profiles/pdf/2016/pdf1/cpa91.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499c5b2f-21a7-40c1-b83f-22e09e12aade",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_renaming = {\n",
    "    \"St.James\": \"St. James\",\n",
    "    \"Weston-Pellam\": \"Weston-Pelham\",\n",
    "}\n",
    "for k, v in d_renaming.items():\n",
    "    gdf[\"AREA_NAME\"] = gdf[\"AREA_NAME\"].str.replace(k, v, regex=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2874974e-e26f-4a8c-a60a-d3dd32b5f009",
   "metadata": {},
   "source": [
    "The incorrect names have been successfully replaced as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1068729f-2c0c-4c8f-a404-87c1e7ee1f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neighbourhood GeoData columns to use\n",
    "geo_cols = [\"AREA_NAME\", \"geometry\", \"Shape__Area\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6457b7e4-f997-4f15-b64e-933e3d4ff0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.query(\"AREA_NAME.str.contains('James Town|Weston-|Cabbage')\")[geo_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbdea8d-87e6-4795-8a52-c670699fac88",
   "metadata": {},
   "source": [
    "Compare manual to provided neighbourhood areas (in square km)\n",
    "- first, changes geodata projection to a cartesian system (EPSG = 3857, in units of m) ([1](https://epsg.io/3857))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32451655-afc5-48a5-8360-1108498a77de",
   "metadata": {},
   "outputs": [],
   "source": [
    "area_diff = (gdf[\"geometry\"].to_crs(epsg=3857).area) - gdf[\"Shape__Area\"]\n",
    "print(area_diff.min(), area_diff.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912969c1-9ae3-4c33-b6e1-fb689f9d52c7",
   "metadata": {},
   "source": [
    "Since these are small differences (in units of square km), we'll use the provided neighbourhood areas from the `Shape__Area` column of the neighbourhood boundary file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7dbec2-090d-4793-89ce-82aeaa110716",
   "metadata": {},
   "source": [
    "### Public Transit Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d05dd59-b7f9-44d1-85de-9e81316fe960",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "params = {\"id\": \"7795b45e-e65a-4465-81fc-c36b9dfff169\"}\n",
    "df_pt_slice = get_public_transit_locations(url, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4901bd6d-a707-4d2b-89c7-89f769b84af6",
   "metadata": {},
   "source": [
    "### Colleges and Universities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797e18c4-9263-4696-b88f-1e2ce353eb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coll_univ = get_coll_univ_locations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac463d8-f005-4d05-a7c6-91a2c1d1718a",
   "metadata": {},
   "source": [
    "### Neighbourhood Profile Data - Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b33e9c-9828-4e6b-9f01-b42a2040fafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "neigh_profile_params = {\"id\": \"6e19a90f-971c-46b3-852c-0c48c436d1fc\"}\n",
    "df_neigh_demog = get_neighbourhood_profile_data(url, neigh_profile_params)\n",
    "df_neigh_demog.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a3e763-d630-496e-b400-dce5b1592040",
   "metadata": {},
   "source": [
    "### Number of Locations Per Neighbourhood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98519b90-93e6-4718-bdf3-59168a78f30b",
   "metadata": {},
   "source": [
    "#### Places of Interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c983f154-07d9-4c63-920d-9b4a524c1377",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_poi[\"ID\"].nunique(), len(df_poi))\n",
    "with pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None):\n",
    "    display(df_poi.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341b88ae-b657-4ad4-ab24-d0a04a229566",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_poi_new = get_data_with_neighbourhood(\n",
    "    gdf[geo_cols],\n",
    "    df_poi.rename(columns={\"POI_LATITUDE\": \"lat\", \"POI_LONGITUDE\": \"lon\",})[\n",
    "        [\"ID\", \"NAME\", \"lat\", \"lon\"]\n",
    "    ],\n",
    "    \"lat\",\n",
    "    \"lon\",\n",
    "    \"ID\",\n",
    ")\n",
    "display(df_poi_new.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cef2a28-b790-4835-b635-c523753f5a1c",
   "metadata": {},
   "source": [
    "#### Cultural Hotspots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbee1c0-7c7f-4834-a858-6eb1888c895a",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert dfch_essentials[\"ID\"].nunique() == len(dfch_essentials)\n",
    "dfch_essentials.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25470ad0-9ca0-4ce1-a06e-bde61230d20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dfch_essentials_new = get_data_with_neighbourhood(\n",
    "    gdf[geo_cols],\n",
    "    dfch_essentials.rename(columns={\"POI_LATITUDE\": \"lat\", \"POI_LONGITUDE\": \"lon\",})[\n",
    "        [\"ID\", \"NAME\", \"lat\", \"lon\"]\n",
    "    ],\n",
    "    \"lat\",\n",
    "    \"lon\",\n",
    "    \"ID\",\n",
    ")\n",
    "display(dfch_essentials_new.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9b1488-c9c1-429f-91ba-22986ceb57ae",
   "metadata": {},
   "source": [
    "#### Colleges and Universities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d1ac0a-51bc-48c9-9065-6e846b07df3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_coll_univ[\"institution_id\"].nunique(), len(df_coll_univ))\n",
    "df_coll_univ.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5312739-d03b-4433-ae6e-3123e3d65503",
   "metadata": {},
   "source": [
    "### Get Neighbourhood Data for Supplementary Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f7f23f-ce02-4d0c-9d40-26619de33437",
   "metadata": {},
   "source": [
    "#### Colleges and Universities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70191394-5d85-4b4c-a828-03bb0cb39eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_coll_univ_new = get_data_with_neighbourhood(\n",
    "    gdf[geo_cols],\n",
    "    df_coll_univ,\n",
    "    \"lat\",\n",
    "    \"lon\",\n",
    "    \"institution_id\",\n",
    ")\n",
    "display(df_coll_univ_new.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992287e3-cdf3-4a8d-b0c9-c61f5308b037",
   "metadata": {},
   "source": [
    "#### Public Transit Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1261fc-c332-44c6-b8aa-1584851b6ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_pt_slice[\"stop_id\"].nunique(), len(df_pt_slice))\n",
    "df_pt_slice.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2966a7ed-e7b2-475c-9ebf-6ba41328d595",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_pt_slice_new = get_data_with_neighbourhood(\n",
    "    gdf[geo_cols],\n",
    "    df_pt_slice,\n",
    "    \"lat\",\n",
    "    \"lon\",\n",
    "    \"stop_id\",\n",
    ")\n",
    "display(df_pt_slice_new.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ae0c16-3229-4990-af89-e9f0c1e7ae01",
   "metadata": {},
   "source": [
    "### Merge Neighbourhood Aggregations with GeoData and Population Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f5be18-a351-4b7b-9c5a-9741f28e0697",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neigh_stats = (\n",
    "    (\n",
    "        gdf.set_index(\"AREA_NAME\")[\n",
    "            [\n",
    "                \"Shape__Area\",\n",
    "                \"Shape__Length\",\n",
    "                \"geometry\",\n",
    "                # \"CLASSIFICATION\",\n",
    "                # \"CLASSIFICATION_CODE\",\n",
    "                \"AREA_LATITUDE\",\n",
    "                \"AREA_LONGITUDE\",\n",
    "            ]\n",
    "        ]\n",
    "        .merge(\n",
    "            df_pt_slice_new.groupby(\"AREA_NAME\")[\"stop_id\"]\n",
    "            .count()\n",
    "            .rename(\"transit_stops\")\n",
    "            .to_frame(),\n",
    "            left_index=True,\n",
    "            right_index=True,\n",
    "            how=\"left\",\n",
    "        )\n",
    "        .merge(\n",
    "            df_coll_univ_new.groupby(\"AREA_NAME\")[\"institution_id\"]\n",
    "            .count()\n",
    "            .rename(\"colleges_univs\")\n",
    "            .to_frame(),\n",
    "            left_index=True,\n",
    "            right_index=True,\n",
    "            how=\"left\",\n",
    "        )\n",
    "        .merge(\n",
    "            dfch_essentials_new.groupby(\"AREA_NAME\")[\"ID\"]\n",
    "            .count()\n",
    "            .rename(\"cultural_attractions\")\n",
    "            .to_frame(),\n",
    "            left_index=True,\n",
    "            right_index=True,\n",
    "            how=\"left\",\n",
    "        )\n",
    "        .merge(\n",
    "            df_poi_new.groupby(\"AREA_NAME\")[\"ID\"]\n",
    "            .count()\n",
    "            .rename(\"places_of_interest\")\n",
    "            .to_frame(),\n",
    "            left_index=True,\n",
    "            right_index=True,\n",
    "            how=\"left\",\n",
    "        )\n",
    "        .fillna(0)\n",
    "        .astype(\n",
    "            {\n",
    "                k: int\n",
    "                for k in [\n",
    "                    \"transit_stops\",\n",
    "                    \"colleges_univs\",\n",
    "                    \"cultural_attractions\",\n",
    "                    \"places_of_interest\",\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "        .merge(\n",
    "            df_neigh_demog.set_index(\"AREA_NAME\")[\n",
    "                [\"Population, 2016\", \"Youth (15-24 years)\", \"Working Age (25-54 years)\"]\n",
    "            ].rename(\n",
    "                columns={\n",
    "                    \"Population, 2016\": \"pop_2016\",\n",
    "                    \"Youth (15-24 years)\": \"youth_15_24\",\n",
    "                    \"Working Age (25-54 years)\": \"work_age_25_54\",\n",
    "                }\n",
    "            ),\n",
    "            left_index=True,\n",
    "            right_index=True,\n",
    "            how=\"left\",\n",
    "        )\n",
    "    )\n",
    "    .add_prefix(\"neigh_\")\n",
    "    .rename(columns={\"neigh_geometry\": \"geometry\"})\n",
    ")\n",
    "df_neigh_stats.columns = df_neigh_stats.columns.str.lower().str.replace(\"__\", \"_\")\n",
    "df_neigh_stats = df_neigh_stats.reset_index()\n",
    "for c in [\"neigh_pop_2016\", \"neigh_youth_15_24\", \"neigh_work_age_25_54\"]:\n",
    "    df_neigh_stats[c] = df_neigh_stats[c].str.replace(\",\", \"\").astype(float)\n",
    "df_neigh_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8aa8066-4707-4c81-be91-ba711a72c102",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert type(df_neigh_stats).__name__ == \"GeoDataFrame\"\n",
    "assert df_stations[\"station_id\"].nunique() == len(df_stations)\n",
    "df_stations.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7881518-ae5a-4e9e-8a27-95ba3249827a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Merge Stations Metadata with Aggregated Neighbourhood Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6280dcf3-1a0d-4c65-87cf-67d5afd00c40",
   "metadata": {},
   "source": [
    "Append the neighbourhood containing each bikeshare station to the station metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2101472-94ed-4059-b86b-3aac18e8c28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_stations_new = get_data_with_neighbourhood(\n",
    "    gdf[geo_cols],\n",
    "    df_stations,\n",
    "    \"lat\",\n",
    "    \"lon\",\n",
    "    \"station_id\",\n",
    ")\n",
    "display(df_stations_new.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611f4067-8676-426c-88a2-9326e885297f",
   "metadata": {},
   "source": [
    "Merge the modified stations metadata with the neighbourhood stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8401bd3-3671-4fb4-8b88-50cd743aaeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stations_new = (\n",
    "    df_stations_new.set_index(\"AREA_NAME\")\n",
    "    .merge(\n",
    "        df_neigh_stats.set_index(\"AREA_NAME\"),\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "        how=\"left\",\n",
    "    )\n",
    "    .reset_index()\n",
    "    .rename(columns={\"Shape__Area\": \"Shape_Area\"})\n",
    ")\n",
    "df_stations_new.columns = df_stations_new.columns.str.upper()\n",
    "print(df_stations_new.shape)\n",
    "with pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None):\n",
    "    display(df_stations_new.head(4))\n",
    "display(df_stations_new.dtypes.rename(\"dtype\").to_frame())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e500cb8-6752-4c80-9589-b40d90d38fb4",
   "metadata": {},
   "source": [
    "## Database Administration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4825a8f4-b190-4010-8834-5cecaac46a37",
   "metadata": {},
   "source": [
    "### Create bikeshare trips and station metadata databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a2ccbd-5dda-4bde-b663-7ff76b7328ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = snowflake.connector.connect(**snowflake_dict_no_db)\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa88164-a9a8-4deb-a776-efc23502d19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for db_name in [trips_db_name, stations_db_name]:\n",
    "    _ = cur.execute(f\"DROP DATABASE IF EXISTS {db_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b633ba2d-ac66-4d9a-84da-d4840ba2a431",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for db_name in [trips_db_name, stations_db_name]:\n",
    "    _ = cur.execute(f\"CREATE DATABASE IF NOT EXISTS {db_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074839b0-c162-4ac8-998b-77cb8e33dcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for db_name in [trips_db_name, stations_db_name]:\n",
    "    _ = show_sql_df(f\"SHOW DATABASES LIKE '{db_name}'\", cur, table_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcfc2c7-1e0b-4e3b-a37f-16b6e40a557d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "_ = show_sql_df(\"SHOW DATABASES\", cur, table_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227a1766-d083-45c0-9e88-0535d3017cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322ae40d-6312-4a53-b377-b0492fda4537",
   "metadata": {},
   "source": [
    "### Create bikeshare trips File Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f01462f-705a-4e17-98d9-6e33d998d215",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = snowflake.connector.connect(**snowflake_dict)\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f199d83-414a-4378-a2ac-427d149ece4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "query = fr\"\"\"\n",
    "        CREATE OR REPLACE FILE FORMAT {trips_file_format_name}\n",
    "        TYPE = 'CSV'\n",
    "        COMPRESSION = 'AUTO'\n",
    "        FIELD_DELIMITER = ','\n",
    "        RECORD_DELIMITER = '\\n'\n",
    "        SKIP_HEADER = 1\n",
    "        TRIM_SPACE = FALSE\n",
    "        ERROR_ON_COLUMN_COUNT_MISMATCH = TRUE\n",
    "        ESCAPE = 'NONE'\n",
    "        DATE_FORMAT = 'AUTO'\n",
    "        TIMESTAMP_FORMAT = 'AUTO'\n",
    "        NULL_IF = ('\\\\N')\n",
    "        \"\"\"\n",
    "_ = cur.execute(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab52ce0c-aa9f-4520-8174-b6013d87b7b0",
   "metadata": {},
   "source": [
    "### Create Internal Stage for bikeshare trips data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed8b3f8-2ce5-4f9f-97fc-c0cf8dc3f3b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "query = f\"\"\"\n",
    "        CREATE OR REPLACE STAGE {trips_stage_name}\n",
    "        FILE_FORMAT = {trips_file_format_name}\n",
    "        \"\"\"\n",
    "_ = cur.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dab740-3ac4-408d-b01e-a97500681466",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "query = \"\"\"\n",
    "        SHOW STAGES\n",
    "        \"\"\"\n",
    "_ = show_sql_df(query, cur, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4224e39d-770c-490d-995f-50dd2902a2f2",
   "metadata": {},
   "source": [
    "### Stage Local Raw Trips Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e95bbad-1640-467a-8077-134b4e3b5088",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for file in glob(\"data/raw/*.csv\"):\n",
    "    query = f\"\"\"\n",
    "            PUT file://{file} @{trips_stage_name}\n",
    "            \"\"\"\n",
    "    print(query.strip())\n",
    "    _ = cur.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83adac2-dfd1-4acf-b102-11c752bc7bcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "        LIST @{trips_stage_name}/\n",
    "        \"\"\"\n",
    "_ = show_sql_df(query, cur, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea58ba5-7fe0-4fc6-9928-293ddd7c31bb",
   "metadata": {},
   "source": [
    "### Create bikeshare trips Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f04d732-03c0-486d-b47e-17a4139c4aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "_ = cur.execute(\n",
    "    f\"\"\"\n",
    "    CREATE OR REPLACE TABLE {trips_table_name} (\n",
    "        trip_id integer,\n",
    "        trip_duration integer,\n",
    "        start_station_id integer,\n",
    "        start_time timestamp,\n",
    "        start_station_name string,\n",
    "        end_station_id integer,\n",
    "        end_time timestamp,\n",
    "        end_station_name string,\n",
    "        bike_id integer,\n",
    "        user_type string\n",
    "    )\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e6a567-f978-4015-a7cf-7aaafdd44d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = show_sql_df(f\"SHOW TABLES LIKE '%{trips_table_name}%'\", cur, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6928aa91-8b3c-4048-ac6c-e192e1ba77bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = show_sql_df(f\"SHOW COLUMNS IN TABLE {trips_table_name}\", cur, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc05768-38ca-48d6-8962-5c2151b229e4",
   "metadata": {},
   "source": [
    "### Add Staged Trips Data to Trips Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae7a09d-8bdc-432f-9e1a-b70bffa354f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "query = f\"\"\"\n",
    "        COPY INTO {trips_table_name} from @{trips_stage_name}\n",
    "        \"\"\"\n",
    "_ = cur.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7234054-b80f-4abb-9360-cc4c91adbb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "query = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM {trips_table_name}\n",
    "        LIMIT 5\n",
    "        \"\"\"\n",
    "_ = show_sql_df(query, cur, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6206afd4-f66d-4c0a-bf00-a02c8c38ae5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "query = f\"\"\"\n",
    "        SELECT COUNT(*) AS num_rows\n",
    "        FROM {trips_table_name}\n",
    "        \"\"\"\n",
    "df_query_nrows_trips = show_sql_df(query, cur, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa97733d-eafd-43f2-a10e-06019fe1f873",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert df_query_nrows_trips.loc[0, \"num_rows\"] == sum(\n",
    "    [l[\"nrows\"] for f in cols_dict_list for k, l in f.items()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46761ac3-d644-465a-bc9b-edad7b86ed2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9aa7937-9dee-4a66-ba49-789e4d58e45b",
   "metadata": {},
   "source": [
    "### Create Stations Metadata to Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81377e07-5179-4aa9-b1b4-1a7a4a67d56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = snowflake.connector.connect(**snowflake_station_stats_dict)\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d450b04-0202-44c7-b33d-cf523179181a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "_ = cur.execute(\n",
    "    f\"\"\"\n",
    "    CREATE OR REPLACE TABLE {station_stats_table_name} (\n",
    "        area_name string,\n",
    "        station_id integer,\n",
    "        name string,\n",
    "        physical_configuration string,\n",
    "        lat float,\n",
    "        lon float,\n",
    "        altitude float,\n",
    "        address string,\n",
    "        capacity integer,\n",
    "        physicalkey integer,\n",
    "        transitcard integer,\n",
    "        creditcard integer,\n",
    "        phone integer,\n",
    "        shape_area float,\n",
    "        neigh_shape_area float,\n",
    "        neigh_shape_length float,\n",
    "        neigh_area_latitude float,\n",
    "        neigh_area_longitude float,\n",
    "        neigh_transit_stops integer,\n",
    "        neigh_colleges_univs integer,\n",
    "        neigh_cultural_attractions integer,\n",
    "        neigh_places_of_interest integer,\n",
    "        neigh_pop_2016 float,\n",
    "        neigh_youth_15_24 float,\n",
    "        neigh_work_age_25_54 float\n",
    "    )\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ca9300-9f78-4d47-8719-fcdbdbb0f273",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = show_sql_df(f\"SHOW TABLES LIKE '%{station_stats_table_name}%'\", cur, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5347e3ab-6b8a-4359-a540-ba5df810d45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cols_stations_table = show_sql_df(\n",
    "    f\"SHOW COLUMNS IN TABLE {station_stats_table_name}\", cur, True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f23b69a-7a27-409b-bd8c-842905b4a03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\n",
    "    len(df_cols_stations_table) == df_stations_new.drop(columns=[\"GEOMETRY\"]).shape[1]\n",
    ")\n",
    "assert pd.Series(\n",
    "    df_stations_new.drop(columns=[\"GEOMETRY\"]).columns.rename(\"column_name\")\n",
    ").equals(df_cols_stations_table[\"column_name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0331ad1b-f95a-4ccd-9c32-c764a5ed6284",
   "metadata": {},
   "source": [
    "### Add Stations Metadata to Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7755b1c1-fc52-4953-92c1-3215a3b7180b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stations_new.drop(columns=[\"GEOMETRY\"]).dtypes.rename(\"dtype\").to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb727057-1ddd-4ad2-9850-a03782fe1dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "success, nchunks, nrows, _ = write_pandas(\n",
    "    conn, df_stations_new.drop(columns=['GEOMETRY']), station_stats_table_name.upper()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de1db45-61c1-4377-9b9e-9f363284c5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "query = f\"\"\"\n",
    "        SELECT COUNT(*) AS num_rows\n",
    "        FROM {station_stats_table_name}\n",
    "        \"\"\"\n",
    "df_query_nrows_stations = show_sql_df(query, cur, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e858a415-aa12-4cff-8616-ba6b557feaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert success\n",
    "try:\n",
    "    assert nrows == len(df_stations_new)\n",
    "    assert df_query_nrows_stations.loc[0, \"num_rows\"] == len(df_stations_new)\n",
    "    print(f\"Exported: {len(df_stations_new):,} rows, as expected\")\n",
    "except AssertionError:\n",
    "    print(f\"Expected: {len(df_stations_new):,} rows\\nActual: {nrows:,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8ddafe-c2ae-49d6-94d8-7a5af711e4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115c5161-4770-4e2f-b82a-f27ce847c118",
   "metadata": {},
   "source": [
    "## Query Data From Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d2e975-8766-407a-8d2d-b75e97926cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = snowflake.connector.connect(**snowflake_dict)\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e48c4e-1372-41e4-8ff4-dfe760dd1124",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "query = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM {trips_db_name}.public.{trips_table_name}\n",
    "        ORDER BY start_time\n",
    "        LIMIT 10\n",
    "        \"\"\"\n",
    "_ = show_sql_df(query, cur, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47312d4-b1df-4f3a-a9da-99755f378230",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "query = f\"\"\"\n",
    "        SELECT trip_id,\n",
    "               trip_duration,\n",
    "               start_time,\n",
    "               start_station_name,\n",
    "               user_type\n",
    "        FROM {trips_db_name}.public.{trips_table_name}\n",
    "        LIMIT 10\n",
    "        \"\"\"\n",
    "_ = show_sql_df(query, cur, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1576e4-5fc3-452a-b3f3-287e2cd695aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "query = f\"\"\"\n",
    "        SELECT start_station_name AS station_name,\n",
    "               DATE_PART(year, start_time) AS year,\n",
    "               DATE_PART(month, start_time) AS month,\n",
    "               DATE_PART(day, start_time) AS day,\n",
    "               DATE_PART(hour, start_time) AS hour,\n",
    "               user_type,\n",
    "               COUNT(DISTINCT trip_id) AS num_trips\n",
    "        FROM {trips_db_name}.public.{trips_table_name}\n",
    "        GROUP BY 1,2,3,4,5,6\n",
    "        LIMIT 10000\n",
    "        \"\"\"\n",
    "_ = show_sql_df(query, cur, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3e27a4-1d17-4935-8030-542c7d10dbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75de70d2-c470-4e73-9edd-b3191c090506",
   "metadata": {},
   "source": [
    "## AWS QuickSight Data Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5675661e-f16b-4cb3-ba02-c03ca7572046",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_arn = [\n",
    "    u[\"Arn\"]\n",
    "    for u in qs_client_user.list_users(AwsAccountId=account_id, Namespace=\"default\")[\n",
    "        \"UserList\"\n",
    "    ]\n",
    "    if u[\"UserName\"].startswith(\"els\")\n",
    "][0]\n",
    "user_arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54d72fc-c505-4509-97ad-207003112f1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "dso_creation_response = qs_client.create_data_source(\n",
    "    AwsAccountId=account_id,\n",
    "    DataSourceId=f\"snowflake-{trips_db_name}\",\n",
    "    Name=trips_db_name,\n",
    "    Type='SNOWFLAKE',\n",
    "    DataSourceParameters={\n",
    "        'SnowflakeParameters': {\n",
    "            'Host': os.getenv(\"SNOWFLAKE_ACCOUNT\")+\".snowflakecomputing.com\",\n",
    "            'Database': trips_db_name.upper(),\n",
    "            'Warehouse': os.getenv(\"SNOWFLAKE_WAREHOUSE\"),\n",
    "        },\n",
    "    },\n",
    "    Credentials={\n",
    "        'CredentialPair': {\n",
    "            'Username': os.getenv(\"SNOWFLAKE_USER\"),\n",
    "            'Password': os.getenv(\"SNOWFLAKE_PASS\"),\n",
    "        },\n",
    "    },\n",
    "    Permissions= [\n",
    "      {\n",
    "        'Principal': user_arn,\n",
    "        'Actions': [\n",
    "          'quicksight:DescribeDataSource',\n",
    "          'quicksight:DescribeDataSourcePermissions',\n",
    "          'quicksight:UpdateDataSource',\n",
    "          'quicksight:UpdateDataSourcePermissions',\n",
    "          'quicksight:DeleteDataSource',\n",
    "          'quicksight:PassDataSource'\n",
    "        ]\n",
    "      }\n",
    "    ],\n",
    ")\n",
    "dso_creation_response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

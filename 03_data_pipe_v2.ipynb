{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ff4e13c-9c5d-4fb2-97d3-720490187b42",
   "metadata": {},
   "source": [
    "# ETL Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f25308-de6b-4f0d-ad5c-7b9763625905",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a14ef9b-319a-44f4-9d64-60ce5e4446fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from prefect import flow, get_run_logger, task\n",
    "from prefect.task_runners import SequentialTaskRunner\n",
    "from prefect.tasks import task_input_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de91c018-fd76-40dc-b54c-7459d857a430",
   "metadata": {},
   "source": [
    "## About"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5a4acf-ccac-4644-b69e-4e147a980db7",
   "metadata": {},
   "source": [
    "Run the ETL workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9c5438-e27d-4aa1-a815-02a7a7af3db8",
   "metadata": {},
   "source": [
    "## User Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c09f97-a6f9-4fb8-8f68-0e60d1cb3278",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "scaling_factor = 6\n",
    "cols_to_scale = [\"A\", \"D\", \"E\"]\n",
    "nrows = [5, 25, 150, 300, 500, 12, 75, 125, 600, 30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4599735b-ecab-4775-9b13-3ee74ffdafe1",
   "metadata": {},
   "source": [
    "## Run Basic Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd423709-f95c-45c7-ac1b-38468f34450f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# @flow(name=\"Testing\")\n",
    "# def basic_flow():\n",
    "#     logger = get_run_logger()\n",
    "#     logger.info(\"The fun is about to begin\")\n",
    "\n",
    "\n",
    "# state = basic_flow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc23539a-b33c-4092-a626-40a760907c2c",
   "metadata": {},
   "source": [
    "## Run ETL Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421e603a-b4fb-4dc9-a9ad-230e1c845fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "@task(\n",
    "    name=\"Data Retriever\",\n",
    "    description=\"Extract raw data.\",\n",
    "    tags=[\"get-data\", \"workflow\"],\n",
    "    cache_key_fn=task_input_hash,\n",
    "    cache_expiration=timedelta(minutes=1),\n",
    ")\n",
    "def extract(nrows: str) -> pd.DataFrame:\n",
    "    \"\"\"Extract data.\"\"\"\n",
    "    logger = get_run_logger()\n",
    "    df = pd.DataFrame(np.random.rand(nrows, 5), columns=list(\"ABCDE\")).assign(\n",
    "        nrows=nrows\n",
    "    )\n",
    "    logger.info(f\"Retrieved {len(df):,} rows of data\")\n",
    "    return df\n",
    "\n",
    "\n",
    "@task(\n",
    "    name=\"Data Transformer\",\n",
    "    description=\"Transform data.\",\n",
    "    tags=[\"transform-data\", \"workflow\"],\n",
    "    cache_key_fn=task_input_hash,\n",
    "    cache_expiration=timedelta(minutes=1),\n",
    ")\n",
    "def transform(\n",
    "    df: pd.DataFrame, scaling_factor: int, cols_to_scale: List[str]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Transform data.\"\"\"\n",
    "    logger = get_run_logger()\n",
    "    df[cols_to_scale] = df[cols_to_scale] * scaling_factor\n",
    "    logger.info(f\"Transformed columns: {','.join(cols_to_scale)}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "@task(\n",
    "    name=\"Data Loader\",\n",
    "    description=\"Load transformed data\",\n",
    "    tags=[\"load-data\", \"workflow\"],\n",
    "    cache_key_fn=task_input_hash,\n",
    "    cache_expiration=timedelta(minutes=1),\n",
    ")\n",
    "def load(data: pd.DataFrame, load_str: str) -> None:\n",
    "    \"\"\"Load data.\"\"\"\n",
    "    logger = get_run_logger()\n",
    "    logger.info(f\"Loaded data with {len(data):,} rows & used str {load_str}.\")\n",
    "\n",
    "\n",
    "@flow(\n",
    "    name=\"ETL Workflow\",\n",
    "    description=\"Run end-to-end ETL flow.\",\n",
    "    version=\"02\",\n",
    "    task_runner=SequentialTaskRunner(),\n",
    ")\n",
    "def data_flow(\n",
    "    nrows: List[int], load_str: str, scaling_factor: int, cols_to_scale: List[str]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Run ETL job.\"\"\"\n",
    "    dfs_trans = []\n",
    "    for nrow in nrows:\n",
    "        df_raw = extract(nrow)\n",
    "        df_transformed = transform(df_raw, scaling_factor, cols_to_scale)\n",
    "        load(df_transformed, load_str)\n",
    "        dfs_trans.append(df_transformed.result())\n",
    "    # print(type(df_transformed.result()))\n",
    "    df_trans = pd.concat(dfs_trans, ignore_index=True)\n",
    "    return df_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ada719-32ee-484f-8473-55c86f662c85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "state = data_flow(nrows, \"dummy-string\", scaling_factor, cols_to_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08226b7-802e-4283-99bf-c776b0c95cd9",
   "metadata": {},
   "source": [
    "## Validate Output of Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7de0c4-88b9-499f-a88b-d16f39f4c6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify output is of the expected datatype\n",
    "assert isinstance(state.result(), pd.DataFrame)\n",
    "\n",
    "# Verify that total number of rows in output is sum of nrows input list\n",
    "assert len(state.result()) == sum(nrows)\n",
    "\n",
    "# Verify scaled columns are in output\n",
    "assert set(cols_to_scale).issubset(set(list(state.result())))\n",
    "\n",
    "# Verify that nrows column contains same values as nrows input list\n",
    "assert state.result()[\"nrows\"].unique().tolist() == nrows"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
